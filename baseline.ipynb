{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from ast import literal_eval\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.general_utils import Progbar\n",
    "from model.base_model import BaseModel\n",
    "from model.config import Config\n",
    "from model.data_utils import minibatches, pad_sequences, \\\n",
    "        load_dataset, one_hot_to_num, get_mean_NDCG\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(BaseModel):\n",
    "    \"\"\"Specialized class of Model for NER\"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, config):\n",
    "        #super(NERModel, self).__init__(config)\n",
    "        super(NERModel, self).__init__(config)\n",
    "        \n",
    "    def add_placeholders(self):\n",
    "        \n",
    "        \"\"\"Define placeholders = entries to computational graph\"\"\"\n",
    "        #self._word_embeddings = tf.placeholder(dtype=tf.float32,\n",
    "        #                         shape=[self.config.nwords, self.config.dim_word]) #(dictionary size, embedding_size)\n",
    "        \n",
    "        # shape = (batch size, max length of sentence in batch)\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"word_ids\")\n",
    "        \n",
    "        # shape = (batch size)\n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None],\n",
    "                        name=\"sequence_lengths\")\n",
    "        \n",
    "        \n",
    "        # shape = (batch size, labels size)\n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, self.config.ntags],\n",
    "                        name=\"labels\")\n",
    "        \n",
    "        # hyper parameters\n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                        name=\"dropout\")\n",
    "        \n",
    "        self.lr = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                        name=\"lr\")\n",
    "        \n",
    "    def get_feed_dict(self, words, labels=None, lr=None, dropout=None):\n",
    "        \n",
    "        \"\"\"Given some data, pad it and build a feed dictionary\n",
    "\n",
    "        Args:\n",
    "            words: list of sentences. A sentence is a list of ids of a list of\n",
    "                words. A word is a list of ids\n",
    "            labels: list of ids\n",
    "            lr: (float) learning rate\n",
    "            dropout: (float) keep prob\n",
    "\n",
    "        Returns:\n",
    "            dict {placeholder: value}\n",
    "\n",
    "        \"\"\"\n",
    "        word_ids, sequence_lengths = pad_sequences(words, 0)\n",
    "        # build feed dictionary\n",
    "        feed = {\n",
    "            self.word_ids: word_ids,\n",
    "            #self._word_embeddings: self.config.embeddings,\n",
    "            self.sequence_lengths: sequence_lengths\n",
    "        }\n",
    "        \n",
    "        if labels is not None:\n",
    "            #labels, _ = pad_sequences(labels, 0)\n",
    "            feed[self.labels] = labels\n",
    "        \n",
    "        if lr is not None:\n",
    "            feed[self.lr] = lr\n",
    "\n",
    "        if dropout is not None:\n",
    "            feed[self.dropout] = dropout\n",
    "            \n",
    "        return feed, sequence_lengths\n",
    "    \n",
    "    def add_word_embeddings_op(self):\n",
    "        \"\"\"Defines self.word_embeddings\n",
    "\n",
    "        If self.config.embeddings is not None and is a np array initialized\n",
    "        with pre-trained word vectors, the word embeddings is just a look-up\n",
    "        and we don't train the vectors. Otherwise, a random matrix with\n",
    "        the correct shape is initialized.\n",
    "        \"\"\"\n",
    "        \n",
    "        _word_embeddings = tf.Variable(\n",
    "                        self.config.embeddings,\n",
    "                        #self._word_embeddings, \n",
    "                        #name=\"_word_embeddings\",\n",
    "                        dtype=tf.float32,\n",
    "                        trainable=self.config.train_embeddings)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #_word_embeddings = tf.Variable(self._word_embeddings)\n",
    "        \n",
    "        \n",
    "        word_embeddings = tf.nn.embedding_lookup(_word_embeddings,\n",
    "                    self.word_ids, name=\"word_embeddings\")\n",
    "                \n",
    "        \n",
    "        self.word_embeddings =  tf.nn.dropout(word_embeddings, self.dropout)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def add_logits_op(self):\n",
    "        \"\"\"Defines self.logits\n",
    "\n",
    "        For each word in each sentence of the batch, it corresponds to a vector\n",
    "        of scores, of dimension equal to the number of tags.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw, cell_bw, self.word_embeddings,\n",
    "                    sequence_length=self.sequence_lengths, dtype=tf.float32)\n",
    "            output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "            output = tf.nn.dropout(output, self.dropout)\n",
    "            \n",
    "        \n",
    "        with tf.variable_scope(\"proj\"):\n",
    "            W = tf.get_variable(\"W\", dtype=tf.float32,\n",
    "                    shape=[2*self.config.hidden_size_lstm, self.config.ntags])\n",
    "\n",
    "            b = tf.get_variable(\"b\", shape=[self.config.ntags],\n",
    "                    dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "            nsteps = tf.shape(output)[1]\n",
    "            output = tf.reshape(output, [-1, 2*self.config.hidden_size_lstm])\n",
    "            pred = tf.matmul(output, W) + b\n",
    "            logits = tf.reshape(pred, [-1, nsteps, self.config.ntags])\n",
    "            \n",
    "            #print ('logits shape', logits)\n",
    "            \n",
    "            self.logits = tf.reduce_sum(logits, axis=1)\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    def add_pred_op(self):\n",
    "        \"\"\"Defines self.labels_pred\n",
    "\n",
    "        This op is defined only in the case where we don't use a CRF since in\n",
    "        that case we can make the prediction \"in the graph\" (thanks to tf\n",
    "        functions in other words). With theCRF, as the inference is coded\n",
    "        in python and not in pure tensroflow, we have to make the prediciton\n",
    "        outside the graph.\n",
    "        \"\"\"\n",
    "        if not self.config.use_crf:\n",
    "            self.labels_pred = tf.cast(tf.argmax(self.logits, axis=-1),\n",
    "                    tf.int32)\n",
    "            \n",
    "            self.one_hot_labels_pred = self.logits\n",
    "    \n",
    "\n",
    "    def add_loss_op(self):\n",
    "        \"\"\"Defines the loss\"\"\"\n",
    "        '''\n",
    "        if self.config.use_crf:\n",
    "            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(\n",
    "                    self.logits, self.labels, self.sequence_lengths)\n",
    "            self.trans_params = trans_params # need to evaluate it for decoding\n",
    "            self.loss = tf.reduce_mean(-log_likelihood)\n",
    "        else:\n",
    "            #losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            #        logits=self.logits, labels=self.labels)\n",
    "        '''\n",
    "        losses = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=self.logits, labels=self.labels)\n",
    "    \n",
    "        #mask = tf.sequence_mask(self.sequence_lengths)\n",
    "        #losses = tf.boolean_mask(losses, mask)\n",
    "        self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        # for tensorboard\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "            \n",
    "    \n",
    "        \n",
    "    def build(self):\n",
    "        # NER specific functions\n",
    "        self.add_placeholders()\n",
    "        self.add_word_embeddings_op()\n",
    "        self.add_logits_op()\n",
    "        self.add_pred_op()\n",
    "        self.add_loss_op()\n",
    "\n",
    "        # Generic functions that add training op and initialize session\n",
    "        self.add_train_op(self.config.lr_method, self.lr, self.loss,\n",
    "                self.config.clip)\n",
    "        self.initialize_session() # now self.sess is defined and vars are init\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_batch(self, words):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            words: list of sentences\n",
    "\n",
    "        Returns:\n",
    "            labels_pred: list of labels for each sentence\n",
    "            sequence_length\n",
    "\n",
    "        \"\"\"\n",
    "        fd, sequence_lengths = self.get_feed_dict(words, dropout=1.0)\n",
    "\n",
    "        if self.config.use_crf:\n",
    "            # get tag scores and transition params of CRF\n",
    "            viterbi_sequences = []\n",
    "            logits, trans_params = self.sess.run(\n",
    "                    [self.logits, self.trans_params], feed_dict=fd)\n",
    "\n",
    "            # iterate over the sentences because no batching in vitervi_decode\n",
    "            for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "                logit = logit[:sequence_length] # keep only the valid steps\n",
    "                viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                        logit, trans_params)\n",
    "                viterbi_sequences += [viterbi_seq]\n",
    "\n",
    "            return viterbi_sequences, sequence_lengths\n",
    "\n",
    "        else:\n",
    "            labels_pred = self.sess.run(self.labels_pred, feed_dict=fd)\n",
    "\n",
    "            return labels_pred, sequence_lengths\n",
    "        \n",
    "    def run_epoch(self, train, dev, epoch):\n",
    "        \"\"\"Performs one complete pass over the train set and evaluate on dev\n",
    "\n",
    "        Args:\n",
    "            train: dataset that yields tuple of sentences, tags\n",
    "            dev: dataset\n",
    "            epoch: (int) index of the current epoch\n",
    "\n",
    "        Returns:\n",
    "            f1: (python float), score to select model on, higher is better\n",
    "\n",
    "        \"\"\"\n",
    "        # progbar stuff for logging\n",
    "        batch_size = self.config.batch_size\n",
    "        nbatches = (len(train) + batch_size - 1) // batch_size\n",
    "        prog = Progbar(target=nbatches)\n",
    "\n",
    "        # iterate over dataset\n",
    "        for i, (words, labels) in enumerate(minibatches(train, batch_size)):\n",
    "            fd, _ = self.get_feed_dict(words, labels, self.config.lr,\n",
    "                    self.config.dropout)\n",
    "\n",
    "            _, train_loss, summary = self.sess.run(\n",
    "                    [self.train_op, self.loss, self.merged], feed_dict=fd)\n",
    "\n",
    "            prog.update(i + 1, [(\"train loss\", train_loss)])\n",
    "\n",
    "            # tensorboard\n",
    "            if i % 10 == 0:\n",
    "                self.file_writer.add_summary(summary, epoch*nbatches + i)\n",
    "\n",
    "        metrics = self.run_evaluate(dev)\n",
    "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "                for k, v in metrics.items()])\n",
    "        self.logger.info(msg)\n",
    "\n",
    "        return metrics[\"f1\"]\n",
    "    \n",
    "    def run_evaluate(self, test, path_to_test=None):\n",
    "        \"\"\"Evaluates performance on test set\n",
    "\n",
    "        Args:\n",
    "            test: dataset that yields tuple of (sentences, tags)\n",
    "\n",
    "        Returns:\n",
    "            metrics: (dict) metrics[\"acc\"] = 98.4, ...\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        pbar = tqdm.tqdm(total=len(test))\n",
    "    \n",
    "        correct_labels, predicted_labels = [], []\n",
    "        for words, labels in minibatches(test, self.config.batch_size):\n",
    "            labels_pred, sequence_lengths = self.predict_batch(words)\n",
    "            correct_labels.extend([one_hot_to_num(label) for label in labels])\n",
    "            predicted_labels.extend(labels_pred)\n",
    "            pbar.update(self.config.batch_size)\n",
    "        pbar.close()    \n",
    "            \n",
    "        acc = accuracy_score(correct_labels, predicted_labels)\n",
    "        f1 = f1_score(correct_labels, predicted_labels, average='macro')\n",
    "            \n",
    "        \n",
    "        if path_to_test is None:\n",
    "            path_to_test = self.config.path_to_test\n",
    "        \n",
    "        test_dataframe = pd.read_csv(path_to_test)\n",
    "        test_dataframe['predicted'] = predicted_labels\n",
    "        \n",
    "        NDCG = get_mean_NDCG(test_dataframe)\n",
    "        \n",
    "        return {\"acc\": 100*acc, \"f1\": 100*f1, \"NDCG\": NDCG}\n",
    "    \n",
    "    def predict(self, words_raw):\n",
    "        \"\"\"Returns list of tags\n",
    "\n",
    "        Args:\n",
    "            words_raw: list of words (string), just one sentence (no batch)\n",
    "\n",
    "        Returns:\n",
    "            preds: list of tags (string), one for each word in the sentence\n",
    "\n",
    "        \"\"\"\n",
    "        words = [self.config.processing_word(w) for w in words_raw]\n",
    "        if type(words[0]) == tuple:\n",
    "            words = zip(*words)\n",
    "        pred_ids, _ = self.predict_batch([words])\n",
    "        preds = [self.idx_to_tag[idx] for idx in list(pred_ids[0])]\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from ast import literal_eval\n",
    "import tqdm\n",
    "from ner_model import NERModel\n",
    "from model.config import Config\n",
    "from model.data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing tf session\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "model = NERModel(config)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_dataset(model.config.path_to_test)\n",
    "val = load_dataset(model.config.path_to_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483/483 [==============================] - 609s - train loss: 1.3029   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9795 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9800it [03:23, 66.72it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting predictions to predicted column in test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc 52.17 - f1 26.97 - NDCG 82295.72\n",
      "- new best score!\n",
      "Epoch 1 training time is 816.6934471130371\n",
      "Epoch 2 out of 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483/483 [==============================] - 776s - train loss: 0.9586   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9795 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9800it [03:21, 68.00it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting predictions to predicted column in test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc 51.70 - f1 27.71 - NDCG 82319.02\n",
      "- new best score!\n",
      "Epoch 2 training time is 982.3980121612549\n"
     ]
    }
   ],
   "source": [
    "model.train(val, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
