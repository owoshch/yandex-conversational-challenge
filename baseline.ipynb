{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from ast import literal_eval\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.general_utils import Progbar\n",
    "from model.base_model import BaseModel\n",
    "from model.config import Config\n",
    "from model.data_utils import minibatches, pad_sequences, \\\n",
    "        load_dataset, one_hot_to_num, get_mean_NDCG\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13495  , -0.21435  ,  0.13176  , ..., -0.59834  , -0.36757  ,\n",
       "         0.18452  ],\n",
       "       [-0.016087 ,  0.1407   ,  0.39573  , ..., -0.27831  ,  0.11708  ,\n",
       "         0.024381 ],\n",
       "       [-0.128    ,  0.14035  ,  0.099967 , ...,  0.22156  , -0.18517  ,\n",
       "        -0.77301  ],\n",
       "       ...,\n",
       "       [-0.32349  , -0.27061  , -0.17653  , ..., -0.17251  , -0.0074593,\n",
       "         0.26135  ],\n",
       "       [-0.13878  ,  0.095609 , -0.60448  , ..., -0.012983 , -0.20404  ,\n",
       "         0.43851  ],\n",
       "       [ 0.36625  ,  0.20512  , -0.1087   , ...,  0.17922  , -0.19327  ,\n",
       "        -0.4266   ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(BaseModel):\n",
    "    \"\"\"Specialized class of Model for NER\"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, config):\n",
    "        #super(NERModel, self).__init__(config)\n",
    "        super(NERModel, self).__init__(config)\n",
    "        \n",
    "    def add_placeholders(self):\n",
    "        \n",
    "        \"\"\"Define placeholders = entries to computational graph\"\"\"\n",
    "        #self._word_embeddings = tf.placeholder(dtype=tf.float32,\n",
    "        #                         shape=[self.config.nwords, self.config.dim_word]) #(dictionary size, embedding_size)\n",
    "        \n",
    "        # shape = (batch size, max length of sentence in batch)\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"word_ids\")\n",
    "        \n",
    "        # shape = (batch size)\n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None],\n",
    "                        name=\"sequence_lengths\")\n",
    "        \n",
    "        \n",
    "        # shape = (batch size, labels size)\n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, self.config.ntags],\n",
    "                        name=\"labels\")\n",
    "        \n",
    "        # hyper parameters\n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                        name=\"dropout\")\n",
    "        \n",
    "        self.lr = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                        name=\"lr\")\n",
    "        \n",
    "    def get_feed_dict(self, words, labels=None, lr=None, dropout=None):\n",
    "        \n",
    "        \"\"\"Given some data, pad it and build a feed dictionary\n",
    "\n",
    "        Args:\n",
    "            words: list of sentences. A sentence is a list of ids of a list of\n",
    "                words. A word is a list of ids\n",
    "            labels: list of ids\n",
    "            lr: (float) learning rate\n",
    "            dropout: (float) keep prob\n",
    "\n",
    "        Returns:\n",
    "            dict {placeholder: value}\n",
    "\n",
    "        \"\"\"\n",
    "        word_ids, sequence_lengths = pad_sequences(words, 0)\n",
    "        # build feed dictionary\n",
    "        feed = {\n",
    "            self.word_ids: word_ids,\n",
    "            #self._word_embeddings: self.config.embeddings,\n",
    "            self.sequence_lengths: sequence_lengths\n",
    "        }\n",
    "        \n",
    "        if labels is not None:\n",
    "            #labels, _ = pad_sequences(labels, 0)\n",
    "            feed[self.labels] = labels\n",
    "        \n",
    "        if lr is not None:\n",
    "            feed[self.lr] = lr\n",
    "\n",
    "        if dropout is not None:\n",
    "            feed[self.dropout] = dropout\n",
    "            \n",
    "        return feed, sequence_lengths\n",
    "    \n",
    "    def add_word_embeddings_op(self):\n",
    "        \"\"\"Defines self.word_embeddings\n",
    "\n",
    "        If self.config.embeddings is not None and is a np array initialized\n",
    "        with pre-trained word vectors, the word embeddings is just a look-up\n",
    "        and we don't train the vectors. Otherwise, a random matrix with\n",
    "        the correct shape is initialized.\n",
    "        \"\"\"\n",
    "        \n",
    "        _word_embeddings = tf.Variable(\n",
    "                        self.config.embeddings,\n",
    "                        #self._word_embeddings, \n",
    "                        #name=\"_word_embeddings\",\n",
    "                        dtype=tf.float32,\n",
    "                        trainable=self.config.train_embeddings)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #_word_embeddings = tf.Variable(self._word_embeddings)\n",
    "        \n",
    "        \n",
    "        word_embeddings = tf.nn.embedding_lookup(_word_embeddings,\n",
    "                    self.word_ids, name=\"word_embeddings\")\n",
    "                \n",
    "        \n",
    "        self.word_embeddings =  tf.nn.dropout(word_embeddings, self.dropout)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def add_logits_op(self):\n",
    "        \"\"\"Defines self.logits\n",
    "\n",
    "        For each word in each sentence of the batch, it corresponds to a vector\n",
    "        of scores, of dimension equal to the number of tags.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw, cell_bw, self.word_embeddings,\n",
    "                    sequence_length=self.sequence_lengths, dtype=tf.float32)\n",
    "            output = tf.concat([self.word_embeddings, output_fw, output_bw], axis=-1)\n",
    "            output = tf.nn.dropout(output, self.dropout)\n",
    "            \n",
    "        with tf.variable_scope(\"proj\"):\n",
    "            W = tf.get_variable(\"W\", dtype=tf.float32,\n",
    "                    shape=[2*self.config.hidden_size_lstm + self.config.dim_word, 150])\n",
    "\n",
    "            b = tf.get_variable(\"b\", shape=[150],\n",
    "                    dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "            nsteps = tf.shape(output)[1]\n",
    "            output = tf.reshape(output, [-1, 2*self.config.hidden_size_lstm + self.config.dim_word])\n",
    "            pred = tf.matmul(output, W) + b\n",
    "            \n",
    "            \n",
    "            pred = tf.nn.relu(pred)\n",
    "            \n",
    "            W1 = tf.get_variable('W1', dtype=tf.float32,\n",
    "                                 shape=[150, 50],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.get_variable('b1', dtype=tf.float32,\n",
    "                                shape=[50], initializer=tf.zeros_initializer())\n",
    "            \n",
    "            pred = tf.matmul(pred, W1) + b1\n",
    "            pred = tf.nn.relu(pred)\n",
    "            \n",
    "            \n",
    "            W2 = tf.get_variable('W2', dtype=tf.float32,\n",
    "                                 shape=[50, self.config.ntags],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.get_variable('b2', dtype=tf.float32,\n",
    "                                shape=[self.config.ntags], initializer=tf.zeros_initializer())\n",
    "            pred = tf.matmul(pred, W2) + b2\n",
    "            pred = tf.nn.sigmoid(pred)\n",
    "            \n",
    "\n",
    "            \n",
    "            logits = tf.reshape(pred, [-1, nsteps, self.config.ntags])\n",
    "            \n",
    "            \n",
    "            \n",
    "            #self.logits = tf.reduce_mean(logits, axis=1)\n",
    "            \n",
    "            logits = logits[:, -4:]\n",
    "            \n",
    "            W3 = tf.get_variable('W3', dtype=tf.float32,\n",
    "                                shape=[self.config.batch_size, 1, 4],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            b3 = tf.get_variable('b3', dtype=tf.float32,\n",
    "                                shape=[self.config.ntags],\n",
    "                                initializer=tf.zeros_initializer())\n",
    "            \n",
    "            logits = tf.matmul(W3, logits) \n",
    "            \n",
    "            logits = tf.reshape(logits, shape=[self.config.batch_size, self.config.ntags])\n",
    "            \n",
    "            self.logits = logits + b2\n",
    "            \n",
    "            \n",
    "            #print ('logits shape', logits.shape)\n",
    "            \n",
    "            #self.logits = tf.reduce_mean(logits, axis=1)\n",
    "    \n",
    "    def add_pred_op(self):\n",
    "        \"\"\"Defines self.labels_pred\n",
    "\n",
    "        This op is defined only in the case where we don't use a CRF since in\n",
    "        that case we can make the prediction \"in the graph\" (thanks to tf\n",
    "        functions in other words). With theCRF, as the inference is coded\n",
    "        in python and not in pure tensroflow, we have to make the prediciton\n",
    "        outside the graph.\n",
    "        \"\"\"\n",
    "        if not self.config.use_crf:\n",
    "            self.labels_pred = tf.cast(tf.argmax(self.logits, axis=-1),\n",
    "                    tf.int32)\n",
    "            \n",
    "            self.one_hot_labels_pred = self.logits\n",
    "    \n",
    "\n",
    "    def add_loss_op(self):\n",
    "        \"\"\"Defines the loss\"\"\"\n",
    "        '''\n",
    "        if self.config.use_crf:\n",
    "            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(\n",
    "                    self.logits, self.labels, self.sequence_lengths)\n",
    "            self.trans_params = trans_params # need to evaluate it for decoding\n",
    "            self.loss = tf.reduce_mean(-log_likelihood)\n",
    "        else:\n",
    "            #losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            #        logits=self.logits, labels=self.labels)\n",
    "        '''\n",
    "        losses = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=self.logits, labels=self.labels)\n",
    "    \n",
    "        #mask = tf.sequence_mask(self.sequence_lengths)\n",
    "        #losses = tf.boolean_mask(losses, mask)\n",
    "        self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        # for tensorboard\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "            \n",
    "    \n",
    "        \n",
    "    def build(self):\n",
    "        # NER specific functions\n",
    "        self.add_placeholders()\n",
    "        self.add_word_embeddings_op()\n",
    "        self.add_logits_op()\n",
    "        self.add_pred_op()\n",
    "        self.add_loss_op()\n",
    "\n",
    "        # Generic functions that add training op and initialize session\n",
    "        self.add_train_op(self.config.lr_method, self.lr, self.loss,\n",
    "                self.config.clip)\n",
    "        self.initialize_session() # now self.sess is defined and vars are init\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_batch(self, words):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            words: list of sentences\n",
    "\n",
    "        Returns:\n",
    "            labels_pred: list of labels for each sentence\n",
    "            sequence_length\n",
    "\n",
    "        \"\"\"\n",
    "        fd, sequence_lengths = self.get_feed_dict(words, dropout=1.0)\n",
    "\n",
    "        if self.config.use_crf:\n",
    "            # get tag scores and transition params of CRF\n",
    "            viterbi_sequences = []\n",
    "            logits, trans_params = self.sess.run(\n",
    "                    [self.logits, self.trans_params], feed_dict=fd)\n",
    "\n",
    "            # iterate over the sentences because no batching in vitervi_decode\n",
    "            for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "                logit = logit[:sequence_length] # keep only the valid steps\n",
    "                viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                        logit, trans_params)\n",
    "                viterbi_sequences += [viterbi_seq]\n",
    "\n",
    "            return viterbi_sequences, sequence_lengths\n",
    "\n",
    "        else:\n",
    "            labels_pred = self.sess.run(self.labels_pred, feed_dict=fd)\n",
    "\n",
    "            return labels_pred, sequence_lengths\n",
    "        \n",
    "    def run_epoch(self, train, dev, epoch):\n",
    "        \"\"\"Performs one complete pass over the train set and evaluate on dev\n",
    "\n",
    "        Args:\n",
    "            train: dataset that yields tuple of sentences, tags\n",
    "            dev: dataset\n",
    "            epoch: (int) index of the current epoch\n",
    "\n",
    "        Returns:\n",
    "            f1: (python float), score to select model on, higher is better\n",
    "\n",
    "        \"\"\"\n",
    "        # progbar stuff for logging\n",
    "        batch_size = self.config.batch_size\n",
    "        nbatches = (len(train) + batch_size - 1) // batch_size\n",
    "        prog = Progbar(target=nbatches)\n",
    "\n",
    "        # iterate over dataset\n",
    "        for i, (words, labels) in enumerate(minibatches(train, batch_size)):\n",
    "            fd, _ = self.get_feed_dict(words, labels, self.config.lr,\n",
    "                    self.config.dropout)\n",
    "\n",
    "            _, train_loss, summary = self.sess.run(\n",
    "                    [self.train_op, self.loss, self.merged], feed_dict=fd)\n",
    "\n",
    "            prog.update(i + 1, [(\"train loss\", train_loss)])\n",
    "\n",
    "            # tensorboard\n",
    "            if i % 10 == 0:\n",
    "                self.file_writer.add_summary(summary, epoch*nbatches + i)\n",
    "\n",
    "        metrics = self.run_evaluate(dev)\n",
    "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "                for k, v in metrics.items()])\n",
    "        self.logger.info(msg)\n",
    "\n",
    "        return metrics[\"f1\"]\n",
    "    \n",
    "    def run_evaluate(self, test, path_to_test=None):\n",
    "        \"\"\"Evaluates performance on test set\n",
    "\n",
    "        Args:\n",
    "            test: dataset that yields tuple of (sentences, tags)\n",
    "\n",
    "        Returns:\n",
    "            metrics: (dict) metrics[\"acc\"] = 98.4, ...\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        pbar = tqdm.tqdm(total=len(test))\n",
    "    \n",
    "        correct_labels, predicted_labels = [], []\n",
    "        for words, labels in minibatches(test, self.config.batch_size):\n",
    "            labels_pred, sequence_lengths = self.predict_batch(words)\n",
    "            correct_labels.extend([one_hot_to_num(label) for label in labels])\n",
    "            predicted_labels.extend(labels_pred)\n",
    "            pbar.update(self.config.batch_size)\n",
    "        pbar.close()    \n",
    "        \n",
    "        print ('saving predicted labels')\n",
    "        np.save(self.config.path_to_predicted_labels, predicted_labels)\n",
    "        print ('predicted labels are saved')\n",
    "        \n",
    "        \n",
    "        acc = accuracy_score(correct_labels, predicted_labels)\n",
    "        f1 = f1_score(correct_labels, predicted_labels, average='macro')\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        if path_to_test is None:\n",
    "            path_to_test = self.config.path_to_test\n",
    "        \n",
    "        \n",
    "        test_dataframe = pd.read_csv(path_to_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        test_dataframe['predicted'] = predicted_labels\n",
    "        \n",
    "        NDCG = get_mean_NDCG(test_dataframe, predicted_labels)\n",
    "        \n",
    "        return {\"acc\": 100*acc, \"f1\": 100*f1, \"NDCG\": NDCG}\n",
    "    \n",
    "    def predict(self, words_raw):\n",
    "        \"\"\"Returns list of tags\n",
    "\n",
    "        Args:\n",
    "            words_raw: list of words (string), just one sentence (no batch)\n",
    "\n",
    "        Returns:\n",
    "            preds: list of tags (string), one for each word in the sentence\n",
    "\n",
    "        \"\"\"\n",
    "        words = [self.config.processing_word(w) for w in words_raw]\n",
    "        if type(words[0]) == tuple:\n",
    "            words = zip(*words)\n",
    "        pred_ids, _ = self.predict_batch([words])\n",
    "        preds = [self.idx_to_tag[idx] for idx in list(pred_ids[0])]\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def create_submission(self, path_to_test=None, \n",
    "               path_to_submission=None):\n",
    "\n",
    "        if path_to_test is None:\n",
    "            path_to_test = self.config.path_to_preprocessed_test\n",
    "        if path_to_submission is None:\n",
    "            path_to_submission = self.config.path_to_submission\n",
    "        \n",
    "        data = pd.read_csv(path_to_test)\n",
    "        \n",
    "        sentences = [literal_eval(sentence) for sentence in data['contexts_and_reply']]\n",
    "        tags = [[0, 0, 0]] * len(sentences)\n",
    "        \n",
    "        test = list(zip(sentences, tags))\n",
    "\n",
    "        pbar = tqdm.tqdm(total=len(test))\n",
    "        predicted_labels = []\n",
    "        for words, labels in minibatches(test, self.config.batch_size):\n",
    "            labels_pred, sequence_lengths = self.predict_batch(words)\n",
    "            predicted_labels.extend(labels_pred)\n",
    "            pbar.update(self.config.batch_size)\n",
    "        pbar.close()    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        data['predicted'] = predicted_labels\n",
    "\n",
    "\n",
    "\n",
    "        context_ids, reply_ids = [], []\n",
    "        for line in data.context_id.unique():\n",
    "            df = data.loc[data['context_id'] == line]\n",
    "            ids_and_labels = list(zip(df.context_id, df.reply_id, df.predicted))\n",
    "            ids_and_labels = sorted(ids_and_labels, key=lambda x: -x[-1])\n",
    "            context_ids.extend([x[0] for x in ids_and_labels])\n",
    "            reply_ids.extend([x[1] for x in ids_and_labels])\n",
    "        with open(path_to_submission, 'w') as fp:\n",
    "            fp.write('\\n'.join('%s %s' % x for x in zip(context_ids, reply_ids)))\n",
    "\n",
    "        print ('Submission is saved')\n",
    "\n",
    "                \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from ast import literal_eval\n",
    "import tqdm\n",
    "#from ner_model import NERModel\n",
    "from model.config import Config\n",
    "from model.data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing tf session\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "model = NERModel(config)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(model.config.path_to_train)\n",
    "test = load_dataset(model.config.path_to_test)\n",
    "val = load_dataset(model.config.path_to_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483/483 [==============================] - 585s - train loss: 1.0062   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9800/9800 [03:21<00:00, 63.10it/s]\n",
      "/Users/kitashov/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting predictions to predicted column in test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f1 22.68 - acc 51.57 - NDCG 82133.37\n",
      "- new best score!\n",
      "Epoch 1 training time is 790.8638257980347\n",
      "Epoch 2 out of 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483/483 [==============================] - 615s - train loss: 0.9612   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9800/9800 [03:20<00:00, 62.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting predictions to predicted column in test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f1 22.68 - acc 51.57 - NDCG 82133.37\n",
      "- new best score!\n",
      "Epoch 2 training time is 820.4658510684967\n"
     ]
    }
   ],
   "source": [
    "model.train(val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.load(\"../data/predicted_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 9800})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
