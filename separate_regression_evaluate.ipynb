{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from ast import literal_eval\n",
    "import tqdm\n",
    "#from separate_NER_model import NERModel\n",
    "from model.separate_config import Config\n",
    "from model.data_utils import load_dataset, load_test_set, \\\n",
    "                            save_submission, sort_xgb_predictions, load_pairwise_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.general_utils import Progbar\n",
    "from model.base_model import BaseModel\n",
    "from model.separate_config import Config\n",
    "from model.data_utils import minibatches, minibatches_w_replies, pad_sequences, \\\n",
    "        load_dataset, load_regression_dataset, one_hot_to_num, \\\n",
    "        get_mean_NDCG, softmax, sort_xgb_predictions, save_submission\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(BaseModel):\n",
    "    \"\"\"Specialized class of Model for NER\"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, config):\n",
    "        #super(NERModel, self).__init__(config)\n",
    "        super(NERModel, self).__init__(config)\n",
    "        \n",
    "    def add_placeholders(self):\n",
    "        \n",
    "        \"\"\"Define placeholders = entries to computational graph\"\"\"\n",
    "        #self._word_embeddings = tf.placeholder(dtype=tf.float32,\n",
    "        #                         shape=[self.config.nwords, self.config.dim_word]) #(dictionary size, embedding_size)\n",
    "        \n",
    "        # shape = (batch size, max length of sentence in batch)\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"word_ids\")\n",
    "        \n",
    "        \n",
    "        self.reply_word_ids = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"word_ids\")\n",
    "        \n",
    "        \n",
    "        # shape = (batch size)\n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None],\n",
    "                        name=\"sequence_lengths\")\n",
    "        \n",
    "        self.replies_lengths = tf.placeholder(tf.int32, shape=[None],\n",
    "                        name=\"sequence_lengths\")\n",
    "        \n",
    "        \n",
    "        # shape = (batch size, labels size)\n",
    "        #self.labels = tf.placeholder(tf.int32, shape=[None, self.config.ntags],\n",
    "        #                name=\"labels\")\n",
    "        \n",
    "        self.labels = tf.placeholder(tf.float32, shape=[None,],\n",
    "                        name=\"labels\")\n",
    "        \n",
    "        # hyper parameters\n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                        name=\"dropout\")\n",
    "        \n",
    "        self.lr = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                        name=\"lr\")\n",
    "        \n",
    "    def get_feed_dict(self, words, replies=None, labels=None, lr=None, dropout=None):\n",
    "        \n",
    "        \"\"\"Given some data, pad it and build a feed dictionary\n",
    "\n",
    "        Args:\n",
    "            words: list of sentences. A sentence is a list of ids of a list of\n",
    "                words. A word is a list of ids\n",
    "            labels: list of ids\n",
    "            lr: (float) learning rate\n",
    "            dropout: (float) keep prob\n",
    "\n",
    "        Returns:\n",
    "            dict {placeholder: value}\n",
    "\n",
    "        \"\"\"\n",
    "        word_ids, sequence_lengths = pad_sequences(words, 0)\n",
    "        \n",
    "        word_reply_ids, sequence_reply_lengths = pad_sequences(replies, 0)\n",
    "        \n",
    "        # build feed dictionary\n",
    "        feed = {\n",
    "            self.word_ids: word_ids,\n",
    "            self.reply_word_ids: word_reply_ids,\n",
    "            #self._word_embeddings: self.config.embeddings,\n",
    "            self.sequence_lengths: sequence_lengths,\n",
    "            self.replies_lengths: sequence_reply_lengths\n",
    "        }\n",
    "        \n",
    "        if labels is not None:\n",
    "            #labels, _ = pad_sequences(labels, 0)\n",
    "            feed[self.labels] = labels\n",
    "        \n",
    "        if lr is not None:\n",
    "            feed[self.lr] = lr\n",
    "\n",
    "        if dropout is not None:\n",
    "            feed[self.dropout] = dropout\n",
    "            \n",
    "        return feed, sequence_lengths\n",
    "    \n",
    "    def add_word_embeddings_op(self):\n",
    "        \"\"\"Defines self.word_embeddings\n",
    "\n",
    "        If self.config.embeddings is not None and is a np array initialized\n",
    "        with pre-trained word vectors, the word embeddings is just a look-up\n",
    "        and we don't train the vectors. Otherwise, a random matrix with\n",
    "        the correct shape is initialized.\n",
    "        \"\"\"\n",
    "        \n",
    "        _word_embeddings = tf.Variable(\n",
    "                        self.config.embeddings,\n",
    "                        #self._word_embeddings, \n",
    "                        #name=\"_word_embeddings\",\n",
    "                        dtype=tf.float32,\n",
    "                        trainable=self.config.train_embeddings)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #_word_embeddings = tf.Variable(self._word_embeddings)\n",
    "        \n",
    "        \n",
    "        word_embeddings = tf.nn.embedding_lookup(_word_embeddings,\n",
    "                    self.word_ids, name=\"word_embeddings\")\n",
    "        \n",
    "        \n",
    "        replies_embeddings = tf.nn.embedding_lookup(_word_embeddings,\n",
    "                    self.reply_word_ids, name=\"replies_embeddings\")\n",
    "                \n",
    "        \n",
    "        self.word_embeddings =  tf.nn.dropout(word_embeddings, self.dropout)\n",
    "        \n",
    "        self.replies_embeddings =  tf.nn.dropout(replies_embeddings, self.dropout)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def add_logits_op(self):\n",
    "        \"\"\"Defines self.logits\n",
    "        For each word in each sentence of the batch, it corresponds to a vector\n",
    "        of scores, of dimension equal to the number of tags.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw, cell_bw, self.word_embeddings,\n",
    "                    sequence_length=self.sequence_lengths, dtype=tf.float32)\n",
    "            output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "            output = tf.nn.dropout(output, self.dropout)\n",
    "            \n",
    "        \n",
    "        with tf.variable_scope(\"proj\"):\n",
    "            W = tf.get_variable(\"W\", dtype=tf.float32,\n",
    "                    shape=[2*self.config.hidden_size_lstm, self.config.hidden_dense_dim])\n",
    "\n",
    "            b = tf.get_variable(\"b\", shape=[self.config.hidden_dense_dim],\n",
    "                    dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "            nsteps = tf.shape(output)[1]\n",
    "            output = tf.reshape(output, [-1, 2*self.config.hidden_size_lstm])\n",
    "            pred = tf.matmul(output, W) + b\n",
    "            logits = tf.reshape(pred, [-1, nsteps, self.config.hidden_dense_dim])\n",
    "            \n",
    "\n",
    "            \n",
    "            logits = tf.reduce_mean(logits, axis=1)\n",
    "            \n",
    "            print ('logits shape', logits.shape)\n",
    "            \n",
    "            \n",
    "            #W1 = tf.get_variable(\"W1\", dtype=tf.float32,\n",
    "            #        shape=[self.config.ntags, 1])\n",
    "            \n",
    "            \n",
    "            #sc = tf.matmul(logits, W1)\n",
    "            \n",
    "            #sc = tf.reshape(sc, (1, -1))[0]\n",
    "                        \n",
    "            #self.logits = sc\n",
    "            \n",
    "        with tf.variable_scope(\"bi-lstm-replies\"):\n",
    "            cell_fw_reply = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            cell_bw_reply = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            (output_fw_reply, output_bw_reply), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw_reply, cell_bw_reply, self.replies_embeddings,\n",
    "                    sequence_length=self.replies_lengths, dtype=tf.float32)\n",
    "            output_reply = tf.concat([output_fw_reply, output_bw_reply], axis=-1)\n",
    "            output_reply = tf.nn.dropout(output_reply, self.dropout)\n",
    "            \n",
    "        with tf.variable_scope(\"proj-replies\"):\n",
    "            W_reply = tf.get_variable(\"W_reply\", dtype=tf.float32,\n",
    "                    shape=[2*self.config.hidden_size_lstm, self.config.hidden_dense_dim])\n",
    "\n",
    "            b_reply = tf.get_variable(\"b_reply\", shape=[self.config.hidden_dense_dim],\n",
    "                    dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "            nsteps = tf.shape(output_reply)[1]\n",
    "            output_reply = tf.reshape(output_reply, [-1, 2*self.config.hidden_size_lstm])\n",
    "            pred_reply = tf.matmul(output_reply, W_reply) + b_reply\n",
    "            logits_reply = tf.reshape(pred_reply, [-1, nsteps, self.config.hidden_dense_dim])\n",
    "            \n",
    "            \n",
    "            logits_reply = tf.reduce_mean(logits_reply, axis=1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            print ('logits reply shape', logits_reply.shape)\n",
    "\n",
    "        self.logits = tf.norm(logits_reply-logits, axis=1, ord='euclidean')\n",
    "            \n",
    "    \n",
    "    \n",
    "    def add_pred_op(self):\n",
    "        \"\"\"Defines self.labels_pred\n",
    "\n",
    "        This op is defined only in the case where we don't use a CRF since in\n",
    "        that case we can make the prediction \"in the graph\" (thanks to tf\n",
    "        functions in other words). With theCRF, as the inference is coded\n",
    "        in python and not in pure tensroflow, we have to make the prediciton\n",
    "        outside the graph.\n",
    "        \"\"\"\n",
    "        if not self.config.use_crf:\n",
    "            #self.labels_pred = tf.cast(tf.argmax(self.logits, axis=-1),\n",
    "            #        tf.int32)\n",
    "            self.labels_pred = self.logits\n",
    "            \n",
    "            #self.one_hot_labels_pred = self.logits\n",
    "    \n",
    "\n",
    "    def add_loss_op(self):\n",
    "        \"\"\"Defines the loss\"\"\"\n",
    "        '''\n",
    "        if self.config.use_crf:\n",
    "            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(\n",
    "                    self.logits, self.labels, self.sequence_lengths)\n",
    "            self.trans_params = trans_params # need to evaluate it for decoding\n",
    "            self.loss = tf.reduce_mean(-log_likelihood)\n",
    "        else:\n",
    "            #losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            #        logits=self.logits, labels=self.labels)\n",
    "        '''\n",
    "        \n",
    "        #losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        #            logits=self.logits, labels=self.labels)\n",
    "        #losses = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        #        logits=self.logits, labels=self.labels)\n",
    "    \n",
    "        #losses = tf.metrics.mean_squared_error(labels=self.labels, predictions=self.logits)\n",
    "        \n",
    "        #mask = tf.sequence_mask(self.sequence_lengths)\n",
    "        #losses = tf.boolean_mask(losses, mask)\n",
    "        \n",
    "        #self.loss, _ = tf.metrics.mean_squared_error(labels=self.labels, predictions=self.logits)\n",
    "        \n",
    "        #losses = tf.metrics.mean_squared_error(labels=self.labels, predictions=self.logits)\n",
    "        #self.loss = tf.reduce_mean(losses)\n",
    "        \n",
    "        losses = tf.square(self.labels - self.logits, name=\"loss\")\n",
    "        self.loss = tf.reduce_mean(losses)\n",
    "        \n",
    "        \n",
    "        # for tensorboard\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "            \n",
    "    \n",
    "        \n",
    "    def build(self):\n",
    "        # NER specific functions\n",
    "        self.add_placeholders()\n",
    "        self.add_word_embeddings_op()\n",
    "        self.add_logits_op()\n",
    "        self.add_pred_op()\n",
    "        self.add_loss_op()\n",
    "\n",
    "        # Generic functions that add training op and initialize session\n",
    "        self.add_train_op(self.config.lr_method, self.lr, self.loss,\n",
    "                self.config.clip)\n",
    "        self.initialize_session() # now self.sess is defined and vars are init\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_batch(self, words, replies):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            words: list of sentences\n",
    "\n",
    "        Returns:\n",
    "            labels_pred: list of labels for each sentence\n",
    "            sequence_length\n",
    "\n",
    "        \"\"\"\n",
    "        fd, sequence_lengths = self.get_feed_dict(words, replies, dropout=1.0)\n",
    "\n",
    "        if self.config.use_crf:\n",
    "            # get tag scores and transition params of CRF\n",
    "            viterbi_sequences = []\n",
    "            logits, trans_params = self.sess.run(\n",
    "                    [self.logits, self.trans_params], feed_dict=fd)\n",
    "\n",
    "            # iterate over the sentences because no batching in vitervi_decode\n",
    "            for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "                logit = logit[:sequence_length] # keep only the valid steps\n",
    "                viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                        logit, trans_params)\n",
    "                viterbi_sequences += [viterbi_seq]\n",
    "\n",
    "            return viterbi_sequences, sequence_lengths\n",
    "\n",
    "        else:\n",
    "            labels_pred = self.sess.run(self.labels_pred, feed_dict=fd)\n",
    "\n",
    "            return labels_pred, sequence_lengths\n",
    "        \n",
    "        \n",
    "    def predict_proba_batch(self, words, replies):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            words: list of sentences\n",
    "\n",
    "        Returns:\n",
    "            labels_pred: list of labels for each sentence\n",
    "            sequence_length\n",
    "\n",
    "        \"\"\"\n",
    "        fd, sequence_lengths = self.get_feed_dict(words, replies, dropout=1.0)\n",
    "\n",
    "        if self.config.use_crf:\n",
    "            # get tag scores and transition params of CRF\n",
    "            viterbi_sequences = []\n",
    "            logits, trans_params = self.sess.run(\n",
    "                    [self.logits, self.trans_params], feed_dict=fd)\n",
    "\n",
    "            # iterate over the sentences because no batching in vitervi_decode\n",
    "            for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "                logit = logit[:sequence_length] # keep only the valid steps\n",
    "                viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                        logit, trans_params)\n",
    "                viterbi_sequences += [viterbi_seq]\n",
    "\n",
    "            return viterbi_sequences, sequence_lengths\n",
    "\n",
    "        else:\n",
    "            labels_pred = self.sess.run(self.labels_pred, feed_dict=fd)\n",
    "        \n",
    "            return labels_pred, sequence_lengths\n",
    "\n",
    "\n",
    "    def predict_proba(self, data):\n",
    "        # data is tuple like while train\n",
    "        final_predictions = []\n",
    "        pbar = tqdm.tqdm(total=len(data))\n",
    "        for words, replies, labels in minibatches_w_replies(data, self.config.batch_size):\n",
    "            labels_pred, sequence_lengths = self.predict_proba_batch(words, replies)\n",
    "            final_predictions.extend(labels_pred)\n",
    "            pbar.update(self.config.batch_size)\n",
    "        pbar.close()\n",
    "        return final_predictions\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def run_epoch(self, train, dev, epoch):\n",
    "        \"\"\"Performs one complete pass over the train set and evaluate on dev\n",
    "\n",
    "        Args:\n",
    "            train: dataset that yields tuple of sentences, tags\n",
    "            dev: dataset\n",
    "            epoch: (int) index of the current epoch\n",
    "\n",
    "        Returns:\n",
    "            f1: (python float), score to select model on, higher is better\n",
    "\n",
    "        \"\"\"\n",
    "        # progbar stuff for logging\n",
    "        batch_size = self.config.batch_size\n",
    "        nbatches = (len(train) + batch_size - 1) // batch_size\n",
    "        prog = Progbar(target=nbatches)\n",
    "\n",
    "        # iterate over dataset\n",
    "        for i, (words, replies, labels) in enumerate(minibatches_w_replies(train, batch_size)):\n",
    "            fd, _ = self.get_feed_dict(words, replies, labels, self.config.lr,\n",
    "                    self.config.dropout)\n",
    "\n",
    "            _, train_loss, summary = self.sess.run(\n",
    "                    [self.train_op, self.loss, self.merged], feed_dict=fd)\n",
    "\n",
    "            prog.update(i + 1, [(\"train loss\", train_loss)])\n",
    "\n",
    "            # tensorboard\n",
    "            if i % 10 == 0:\n",
    "                self.file_writer.add_summary(summary, epoch*nbatches + i)\n",
    "\n",
    "        metrics = self.run_evaluate(dev)\n",
    "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "                for k, v in metrics.items()])\n",
    "        self.logger.info(msg)\n",
    "\n",
    "        return metrics[\"NDCG\"] / 100000\n",
    "    \n",
    "    def run_evaluate(self, test, path_to_test=None):\n",
    "        \"\"\"Evaluates performance on test set\n",
    "\n",
    "        Args:\n",
    "            test: dataset that yields tuple of (sentences, tags)\n",
    "\n",
    "        Returns:\n",
    "            metrics: (dict) metrics[\"acc\"] = 98.4, ...\n",
    "\n",
    "        \"\"\"\n",
    "        if path_to_test == None:\n",
    "            path_to_test = self.config.path_to_test\n",
    "        \n",
    "        pbar = tqdm.tqdm(total=len(test))\n",
    "        \n",
    "        predicted_labels = []\n",
    "        for words, replies, labels in minibatches_w_replies(test, self.config.batch_size):\n",
    "            labels_pred, sequence_lengths = self.predict_batch(words, replies)\n",
    "            predicted_labels.extend(labels_pred)\n",
    "            pbar.update(self.config.batch_size)\n",
    "        pbar.close()  \n",
    "        \n",
    "        test_dataframe = pd.read_csv(path_to_test)\n",
    "        \n",
    "        sorted_preds = sort_xgb_predictions(test_dataframe, predicted_labels) \n",
    "        \n",
    "        print ('sorted preds', sorted_preds[:20])\n",
    "        print ('type:', type(sorted_preds[0]))\n",
    "        \n",
    "        NDCG = get_mean_NDCG(test_dataframe, sorted_preds)\n",
    "        return {\"NDCG\": NDCG}\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    def predict(self, words_raw):\n",
    "        \"\"\"Returns list of tags\n",
    "\n",
    "        Args:\n",
    "            words_raw: list of words (string), just one sentence (no batch)\n",
    "\n",
    "        Returns:\n",
    "            preds: list of tags (string), one for each word in the sentence\n",
    "\n",
    "        \"\"\"\n",
    "        words = [self.config.processing_word(w) for w in words_raw]\n",
    "        if type(words[0]) == tuple:\n",
    "            words = zip(*words)\n",
    "        pred_ids, _ = self.predict_batch([words])\n",
    "        preds = [self.idx_to_tag[idx] for idx in list(pred_ids[0])]\n",
    "\n",
    "        return preds\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from ast import literal_eval\n",
    "import tqdm\n",
    "#from ner_model import NERModel\n",
    "from model.separate_config import Config\n",
    "from model.data_utils import load_dataset, load_test_set, load_pairwise_dataset, load_regression_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape (?, 300)\n",
      "logits reply shape (?, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing tf session\n"
     ]
    }
   ],
   "source": [
    "model = NERModel(config)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/separate_regression/model.weights/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.dir_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reloading the latest trained model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from results/separate_regression/model.weights/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from results/separate_regression/model.weights/\n"
     ]
    }
   ],
   "source": [
    "model.restore_session(config.dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub = load_test_set(model.config.path_to_preprocessed_test)\n",
    "sub = load_pairwise_testset(model.config.path_to_preprocessed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9980it [06:56, 62.36it/s]                          \n"
     ]
    }
   ],
   "source": [
    "sub_preds = model.predict_proba(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "sub_df = pd.read_csv(model.config.path_to_preprocessed_test)\n",
    "save_submission(\"../data/nn_separate_regression_train_1_epoch.txt\", sub_df, sort_xgb_predictions(sub_df, sub_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
